{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0a7e22-b6df-4980-9ce0-32cb93c9ea3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question 1: What is a Decision Tree, and how does it work in the context of\n",
    "classification?\n",
    "\n",
    "A Decision Tree is a type of supervised machine learning algorithm used for classification and regression tasks. In the context of classification, it works by learning decision rules from the features of the data to predict the class labels of new instances.\n",
    "\n",
    " How It Works (Classification Context):\n",
    "Tree Structure:\n",
    "\n",
    "It is structured like a flowchart:\n",
    "\n",
    "Root Node: Represents the entire dataset and starts the splitting.\n",
    "\n",
    "Internal Nodes: Represent a decision based on a feature.\n",
    "\n",
    "Leaf Nodes: Represent the final class label or outcome.\n",
    "\n",
    "Splitting the Data:\n",
    "\n",
    "At each node, the algorithm chooses the best feature and threshold to split the data based on a criterion such as:\n",
    "\n",
    "Gini Impurity\n",
    "\n",
    "Entropy / Information Gain\n",
    "\n",
    "The goal is to create groups that are as pure as possible (mostly containing instances of a single class).\n",
    "\n",
    "Recursive Partitioning:\n",
    "\n",
    "The process continues recursively:\n",
    "\n",
    "Each subset is split further until a stopping condition is met (e.g., maximum depth, minimum number of samples, or purity of nodes).\n",
    "\n",
    "Classification:\n",
    "\n",
    "To classify a new instance, the algorithm traverses the tree from the root to a leaf, making decisions based on the input features at each node.\n",
    "\n",
    "The class label at the leaf node is the predicted class.\n",
    "\n",
    "\n",
    "Suppose you're classifying whether a person will buy a product based on age and income.\n",
    "\n",
    "csharp\n",
    "Copy\n",
    "Edit\n",
    "          [Age < 30?]\n",
    "             /    \\\n",
    "         Yes       No\n",
    "        /            \\\n",
    "  [Income > 50K?]    [Buy=Yes]\n",
    "     /      \\\n",
    " Buy=No     Buy=Yes\n",
    "You start at the root and follow the path based on feature values until you reach a decision.\n",
    " Pros:\n",
    "Easy to interpret and visualize\n",
    "\n",
    "Handles both numerical and categorical data\n",
    "\n",
    "Requires little data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03249def-7989-4183-9b82-8b6332db9816",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question 2: Explain the concepts of Gini Impurity and Entropy as impurity measures.\n",
    "How do they impact the splits in a Decision Tree?\n",
    "\n",
    " Gini Impurity and Entropy are two commonly used impurity measures in decision trees. They help the algorithm decide how to split the data at each node by quantifying how \"pure\" or \"impure\" a set of class labels is.\n",
    " Gini Impurity measures the probability that a randomly chosen sample would be incorrectly classified if it was randomly labeled according to the distribution of class labels in the subset\n",
    " How They Impact Splits in a Decision Tree\n",
    "At each node, the decision tree algorithm:\n",
    "\n",
    "Calculates Gini Impurity or Entropy for all possible splits on all features.\n",
    "\n",
    "Selects the split that results in the largest decrease in impurity (called Information Gain when using Entropy).\n",
    "\n",
    "The goal is to make child nodes as pure as possible:\n",
    "\n",
    "Gini prefers larger class separation\n",
    "\n",
    "Entropy is more sensitive to changes in class distribution\n",
    "| Criterion | Gini Impurity               | Entropy (Information Gain)        |\n",
    "| --------- | --------------------------- | --------------------------------- |\n",
    "| Formula   | $1 - \\sum p_i^2$            | $-\\sum p_i \\log_2(p_i)$           |\n",
    "| Range     | 0 (pure) to \\~0.5 (binary)  | 0 (pure) to 1 (binary)            |\n",
    "| Speed     | Faster (no log computation) | Slightly slower                   |\n",
    "| Behavior  | Prefers larger splits       | More sensitive to class imbalance |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a771fc-b518-42c7-8c79-97bf525d74ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question 3: What is the difference between Pre-Pruning and Post-Pruning in Decision\n",
    "Trees? Give one practical advantage of using each.\n",
    "Pre-pruning and Post-pruning are two techniques used to prevent overfitting in decision trees by limiting their size or complexity.\n",
    "\n",
    "1. Pre-Pruning (a.k.a. Early Stopping)\n",
    " Definition:\n",
    "Pre-pruning stops the tree from growing once a certain condition is met before the tree reaches full depth.\n",
    "\n",
    " How It Works:\n",
    "During the tree-building process, the algorithm checks:\n",
    "\n",
    "Maximum tree depth\n",
    "\n",
    "Minimum number of samples required to split\n",
    "\n",
    "Minimum gain in impurity\n",
    "\n",
    "Maximum number of leaf nodes\n",
    "\n",
    "If the condition is met, the split is not made, and the node becomes a leaf.\n",
    "\n",
    " Practical Advantage:\n",
    "Faster training — Since the tree is built only to a limited depth, it's quicker and more efficient for large datasets.\n",
    "\n",
    "2. Post-Pruning (a.k.a. Cost Complexity Pruning)\n",
    " Definition:\n",
    "Post-pruning allows the tree to grow fully, then trims it back by removing branches that do not improve performance on a validation set.\n",
    "\n",
    " How It Works:\n",
    "After the tree is grown:\n",
    "\n",
    "Evaluate subtrees using a validation set or cross-validation\n",
    "\n",
    "Remove branches that have little or no impact on accuracy\n",
    "\n",
    "This simplifies the model while preserving performance\n",
    "\n",
    " Practical Advantage:\n",
    "Better generalization — By evaluating based on actual performance, post-pruning often results in more accurate models on unseen data.\n",
    "\n",
    "Summary Table\n",
    "Feature\tPre-Pruning\tPost-Pruning\n",
    "When Applied\tDuring tree growth\tAfter full tree is built\n",
    "Basis\tHeuristics/thresholds\tValidation set or error analysis\n",
    "Speed\tFaster training\tSlower training\n",
    "Accuracy\tMight stop too early\tUsually better generalization\n",
    "Example Param\tmax_depth, min_samples_split\tCost-complexity pruning (ccp_alpha in scikit-learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc52970b-ac90-4d86-a046-8e55f60d9fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question 4: What is Information Gain in Decision Trees, and why is it important for\n",
    "choosing the best split?\n",
    "Information Gain (IG) measures the reduction in entropy (or impurity) achieved by splitting a dataset based on a particular feature. It tells us how much \"information\" a feature gives us about the class label.\n",
    "\n",
    "In simple terms:\n",
    "\n",
    "Information Gain = Entropy (before split) – Weighted Entropy (after split)\n",
    "\n",
    "\n",
    "\n",
    "Why Is Information Gain Important?\n",
    " It helps choose the best feature to split on at each step.\n",
    "The feature with the highest Information Gain is chosen because it provides the most significant reduction in uncertainty.\n",
    "\n",
    "This leads to purer child nodes and helps the tree learn faster and more accurately.\n",
    "\n",
    " Example:\n",
    "Imagine you're classifying if someone will buy a product based on \"Age\":\n",
    "\n",
    "Before Split:\n",
    "Mixed classes (Entropy = 0.94)\n",
    "\n",
    "After Splitting on \"Age\":\n",
    "One group has mostly \"Yes\" (low entropy)\n",
    "\n",
    "Another group has mostly \"No\" (low entropy)\n",
    "\n",
    "Then:\n",
    "Information Gain is high, so \"Age\" is a good feature for splitting.\n",
    "\n",
    " Low Information Gain?\n",
    "If a split doesn't reduce entropy much (i.e., child nodes are still mixed), then the Information Gain is low, and the feature is not helpful for classification.\n",
    "\n",
    "| Concept              | Description                                |\n",
    "| -------------------- | ------------------------------------------ |\n",
    "| **Information Gain** | Reduction in entropy after a dataset split |\n",
    "| **Used For**         | Selecting the best feature to split a node |\n",
    "| **Goal**             | Maximize Information Gain → Improve purity |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab90de0-d4a7-4b48-acde-45a51330e114",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question 5: What are some common real-world applications of Decision Trees, and\n",
    "what are their main advantages and limitations?\n",
    "Medical Diagnosis\n",
    "\n",
    "Used to diagnose diseases based on symptoms and patient history.\n",
    "\n",
    "Example: Predicting whether a tumor is malignant or benign.\n",
    "\n",
    "Credit Scoring and Risk Assessment\n",
    "\n",
    "Banks and financial institutions use decision trees to assess loan eligibility and creditworthiness.\n",
    "\n",
    "Customer Relationship Management (CRM)\n",
    "\n",
    "Helps identify potential customer churn, segment customers, and predict customer lifetime value.\n",
    "\n",
    "Fraud Detection\n",
    "\n",
    "Used to detect unusual patterns in transactions that may indicate fraud.\n",
    "\n",
    "Marketing and Sales\n",
    "\n",
    "Helps in targeting customers by predicting their response to marketing campaigns.\n",
    "\n",
    "Manufacturing and Quality Control\n",
    "\n",
    "Used for decision-making in production processes, predicting equipment failures, and quality assurance.\n",
    "\n",
    "Human Resources\n",
    "\n",
    "Assists in employee performance evaluation and predicting employee attrition.\n",
    "\n",
    "Main Advantages of Decision Trees:\n",
    "\n",
    "Easy to Understand and Interpret: The tree structure is visual and intuitive.\n",
    "\n",
    "Requires Little Data Preparation: No need for feature scaling or normalization.\n",
    "\n",
    "Handles Both Numerical and Categorical Data.\n",
    "\n",
    "Non-parametric: Makes no assumptions about the data distribution.\n",
    "\n",
    "Works Well with Large Datasets: Especially useful for classification and regression problems.\n",
    "\n",
    "Main Limitations of Decision Trees:\n",
    "\n",
    "Prone to Overfitting: Especially with deep trees and noisy data.\n",
    "\n",
    "Unstable: Small changes in data can lead to a completely different tree.\n",
    "\n",
    "Biased with Imbalanced Datasets: May favor classes with more samples.\n",
    "\n",
    "Can Be Less Accurate Than Ensemble Methods: Like Random Forests or Gradient Boosted Trees.\n",
    "\n",
    "Greedy Algorithms: Decision trees use a greedy approach that may not lead to the globally optimal tree.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d96867-d141-4630-b59d-c779c3093bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "● Iris Dataset for classification tasks (sklearn.datasets.load_iris() or\n",
    "provided CSV).\n",
    "● Boston Housing Dataset for regression tasks\n",
    "(sklearn.datasets.load_boston() or provided CSV).\n",
    "Question 6: Write a Python program to:\n",
    "● Load the Iris Dataset\n",
    "● Train a Decision Tree Classifier using the Gini criterion\n",
    "● Print the model’s accuracy and feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e123b8e-0aa7-4e85-817e-c2f318cef093",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1339389186.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    pip install scikit-learn\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split into training and testing datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize and train Decision Tree Classifier with Gini criterion\n",
    "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Print results\n",
    "print(\"Model Accuracy: {:.2f}%\".format(accuracy * 100))\n",
    "print(\"Feature Importances:\")\n",
    "\n",
    "for feature, importance in zip(iris.feature_names, clf.feature_importances_):\n",
    "    print(f\"- {feature}: {importance:.4f}\")\n",
    "\n",
    "Model Accuracy: 100.00%\n",
    "Feature Importances:\n",
    "- sepal length (cm): 0.0000\n",
    "- sepal width (cm): 0.0000\n",
    "- petal length (cm): 0.6667\n",
    "- petal width (cm): 0.3333\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f62a06-f1c0-4665-8826-ac97bcbbea92",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question 7: Write a Python program to:\n",
    "● Load the Iris Dataset\n",
    "● Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to\n",
    "a fully-grown tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0772bfe3-c4a8-44f0-a7c7-690e710aa99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Fully-grown tree (no depth limit)\n",
    "clf_full = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
    "clf_full.fit(X_train, y_train)\n",
    "y_pred_full = clf_full.predict(X_test)\n",
    "acc_full = accuracy_score(y_test, y_pred_full)\n",
    "\n",
    "# Tree with max_depth=3\n",
    "clf_depth3 = DecisionTreeClassifier(criterion='gini', max_depth=3, random_state=42)\n",
    "clf_depth3.fit(X_train, y_train)\n",
    "y_pred_depth3 = clf_depth3.predict(X_test)\n",
    "acc_depth3 = accuracy_score(y_test, y_pred_depth3)\n",
    "\n",
    "# Print the results\n",
    "print(\"Accuracy of Fully-grown Tree     : {:.2f}%\".format(acc_full * 100))\n",
    "print(\"Accuracy of Tree with max_depth=3: {:.2f}%\".format(acc_depth3 * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d7a1ad-f74e-4392-b29d-e476605b8822",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question 8: Write a Python program to:\n",
    "● Load the Boston Housing Dataset\n",
    "● Train a Decision Tree Regressor\n",
    "● Print the Mean Squared Error (MSE) and feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e626a33-431d-4c2c-a177-7f9e9e21d410",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load the Boston Housing dataset\n",
    "boston = load_boston()\n",
    "X = boston.data\n",
    "y = boston.target\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train Decision Tree Regressor\n",
    "regressor = DecisionTreeRegressor(random_state=42)\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = regressor.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# Print results\n",
    "print(\"Mean Squared Error (MSE): {:.2f}\".format(mse))\n",
    "print(\"Feature Importances:\")\n",
    "\n",
    "for feature, importance in zip(boston.feature_names, regressor.feature_importances_):\n",
    "    print(f\"- {feature}: {importance:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e498e6ad-2216-465c-a637-c4afbd95ea62",
   "metadata": {},
   "outputs": [],
   "source": [
    "Mean Squared Error (MSE): 10.35\n",
    "Feature Importances:\n",
    "- CRIM: 0.0000\n",
    "- ZN: 0.0000\n",
    "- INDUS: 0.0000\n",
    "- CHAS: 0.0000\n",
    "- NOX: 0.0235\n",
    "- RM: 0.7041\n",
    "- AGE: 0.0000\n",
    "- DIS: 0.0000\n",
    "- RAD: 0.0000\n",
    "- TAX: 0.0160\n",
    "- PTRATIO: 0.0164\n",
    "- B: 0.0356\n",
    "- LSTAT: 0.2044\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a855d16d-d007-4ca4-8a42-4ca31f20ad1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question 9: Write a Python program to:\n",
    "● Load the Iris Dataset\n",
    "● Tune the Decision Tree’s max_depth and min_samples_split using\n",
    "GridSearchCV\n",
    "● Print the best parameters and the resulting model accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89131f46-7d86-41e1-93be-f9be5dd30d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [2, 3, 4, 5, None],\n",
    "    'min_samples_split': [2, 4, 6, 8]\n",
    "}\n",
    "\n",
    "# Create Decision Tree Classifier\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Perform Grid Search with 5-fold cross-validation\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = best_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Output results\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Model Accuracy: {:.2f}%\".format(accuracy * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571b320b-a01e-4354-bcdf-012e7dd344ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "Best Parameters: {'max_depth': 3, 'min_samples_split': 2}\n",
    "Model Accuracy: 100.00%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bfcd60-d7b6-4e25-93bf-633492ea850a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question 10: Imagine you’re working as a data scientist for a healthcare company that\n",
    "wants to predict whether a patient has a certain disease. You have a large dataset with\n",
    "mixed data types and some missing values.\n",
    "Explain the step-by-step process you would follow to:\n",
    "● Handle the missing values\n",
    "● Encode the categorical features\n",
    "● Train a Decision Tree model\n",
    "● Tune its hyperparameters\n",
    "● Evaluate its performance\n",
    "And describe what business value this model could provide in the real-world\n",
    "setting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7841a9-27e0-47f9-b948-d6aefbb77152",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Handle Missing Values\n",
    "Numerical Features:\n",
    "\n",
    "Use mean or median imputation based on distribution.\n",
    "\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "num_imputer = SimpleImputer(strategy='median')\n",
    "Categorical Features:\n",
    "\n",
    "Use most frequent (mode) or a placeholder like \"Unknown\".\n",
    "\n",
    "\n",
    "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "Use ColumnTransformer or Pipeline to apply imputation separately to numerical and categorical columns.\n",
    "\n",
    "2. Encode the Categorical Features\n",
    "Use One-Hot Encoding for nominal categories:\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "Use Ordinal Encoding if categories have order (e.g., low/medium/high):\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "Again, use ColumnTransformer to handle different columns appropriately.\n",
    "\n",
    "3. Train a Decision Tree Model\n",
    "After preprocessing, train the model:\n",
    "\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "Use a pipeline to chain preprocessing and model training:\n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "4. Tune Hyperparameters\n",
    "Use GridSearchCV to tune:\n",
    "\n",
    "max_depth\n",
    "\n",
    "min_samples_split\n",
    "\n",
    "min_samples_leaf\n",
    "\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'model__max_depth': [3, 5, 10, None],\n",
    "    'model__min_samples_split': [2, 5, 10],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "5. Evaluate Model Performance\n",
    "Use accuracy, precision, recall, and F1-score (especially important in healthcare!):\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = grid_search.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "For imbalanced datasets, consider ROC-AUC and confusion matrix.\n",
    "\n",
    "Business Value of the Model in Healthcare\n",
    "Early Disease Detection\n",
    "\n",
    "Supports doctors in identifying patients at risk even before severe symptoms appear.\n",
    "\n",
    "Personalized Treatment Plans\n",
    "\n",
    "Enables prioritization of patients based on predicted disease risk.\n",
    "\n",
    "Cost Reduction\n",
    "\n",
    "Reduces unnecessary lab tests and hospitalizations through accurate triage.\n",
    "\n",
    "Operational Efficiency\n",
    "\n",
    "Helps hospitals allocate resources more effectively by predicting patient needs.\n",
    "\n",
    "Regulatory Reporting & Insights\n",
    "\n",
    "Provides explainable and auditable results (important for compliance and trust).\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
